import asyncio
from playwright.async_api import async_playwright
import os
import requests
import re
from datetime import datetime, timedelta, timezone

# === Config ===
url = "https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/saas-change-log.htm"
state_file = "last_seen_hub.txt"
chat_webhook = os.environ.get("GOOGLE_CHAT_WEBHOOK")
days_to_look_back = 7

if not chat_webhook:
    raise ValueError("âŒ GOOGLE_CHAT_WEBHOOK not set!")

# === Parse US-style dates like 4/3/2025 or 12/25/2025
def parse_date(text):
    match = re.match(r"^(\d{1,2})/(\d{1,2})/(\d{4})", text)
    if match:
        month, day, year = map(int, match.groups())
        return datetime(year, month, day, tzinfo=timezone.utc)
    return None

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(url, timeout=60000)

        # Optional: wait for rendering
        await page.wait_for_timeout(3000)
        await page.wait_for_selector("h2", timeout=15000, state="attached")

        now = datetime.now(timezone.utc)
        seen_entries = set()

        if os.path.exists(state_file):
            with open(state_file, "r") as f:
                seen_entries = set(line.strip() for line in f if line.strip())

        print(f"ğŸ§  Loaded {len(seen_entries)} previously seen entries.")

        # Grab only paragraphs directly following h2
        paragraphs = await page.locator("h2 + p").all()
        print(f"ğŸ” Scanning {len(paragraphs)} paragraphs under h2 headers.")

        if not paragraphs:
            print("âš ï¸ No <p> elements found following <h2> â€” page layout may have changed.")

        date_pattern = r"\d{1,2}/\d{1,2}/\d{4}"
        updates = []

        for p in paragraphs:
            p_text = re.sub(r"\s+", " ", (await p.text_content()).strip())
            print(f"ğŸ‘€ Raw paragraph: {p_text[:80]}")

            date_match = re.search(date_pattern, p_text)
            if date_match:
                found_date = date_match.group()
                print(f"ğŸ“… Found date: {found_date} in â†’ {p_text[:60]}")
            else:
                print(f"ğŸš« Skipped (no date): {p_text[:60]}")
                continue

            post_date = parse_date(found_date)
            if not post_date:
                continue

            if post_date < now - timedelta(days=days_to_look_back):
                print(f"ğŸ“­ Skipping old post: {found_date}")
                continue

            h2 = p.locator("xpath=preceding-sibling::h2[1]")
            if await h2.count() == 0:
                continue

            title = (await h2.text_content()).strip()
            unique_id = f"{title} - {found_date}"

            if unique_id in seen_entries:
                print(f"â†ªï¸ Already seen: {unique_id}")
                continue

            updates.append({
                "title": title,
                "date": found_date,
                "summary": p_text,
                "id": unique_id
            })

        if not updates:
            print("âœ… No new updates found.")
        else:
            print(f"ğŸ“¦ Sending {len(updates)} new update(s)...")

        for update in updates:
            message = {
                "text": f"ğŸ“¢ *New Qlik Hub Update!*\n\n*{update['title']}*\nğŸ—“ï¸ {update['date']}\n{update['summary']}\nğŸ”— {url}"
            }

            res = requests.post(chat_webhook, json=message)
            if res.status_code == 200:
                print(f"âœ… Sent: {update['id']}")
                seen_entries.add(update['id'])
            else:
                print(f"âŒ Failed to send: {res.status_code} - {res.text}")

        with open(state_file, "w") as f:
            for entry in seen_entries:
                f.write(entry + "\n")

        await browser.close()

asyncio.run(main())
